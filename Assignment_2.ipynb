{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Rough Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, log_loss\n",
    "from pandas.api.types import CategoricalDtype\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sci.crypt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sci.electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0\n",
       "0  comp.sys.ibm.pc.hardware\n",
       "1     comp.sys.mac.hardware\n",
       "2                 rec.autos\n",
       "3           rec.motorcycles\n",
       "4                 sci.crypt\n",
       "5           sci.electronics\n",
       "6    soc.religion.christian\n",
       "7        talk.religion.misc"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create data frames\n",
    "data_path = os.path.join(os.getcwd(), 'Data', 'PartA', '20ng_train.csv')\n",
    "trn_newsgroup = pd.read_csv(data_path, delimiter=',', compression='bz2')\n",
    "data_path2 = os.path.join(os.getcwd(), 'Data', 'PartA', '20ng_test.csv')\n",
    "tst_newsgroup = pd.read_csv(data_path2, delimiter=',', compression='bz2')\n",
    "data_path3 = os.path.join(os.getcwd(), 'Data', 'PartA', '20ng_labels.csv')\n",
    "lbl_newsgroup = pd.read_csv(data_path3, delimiter=',', compression='bz2')\n",
    "lbl_newsgroup.head\n",
    "(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop class labels\n",
    "trn_newsgroup_targets = trn_newsgroup['class']\n",
    "trn_newsgroup = trn_newsgroup.drop(['class'], axis = 1)\n",
    "tst_newsgroup_targets = tst_newsgroup['class']\n",
    "tst_newsgroup = tst_newsgroup.drop(['class'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train rows: 5648\n",
      "Number of train columns: 1000\n",
      "Number of test rows: 1883\n",
      "Number of test columns: 1000\n",
      "737\n",
      "722\n",
      "742\n",
      "747\n",
      "743\n",
      "738\n",
      "748\n",
      "471\n",
      "5648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     3\n",
       "1     2\n",
       "2     1\n",
       "3     1\n",
       "4     7\n",
       "5     4\n",
       "6     4\n",
       "7     2\n",
       "8     6\n",
       "9     1\n",
       "10    1\n",
       "11    0\n",
       "12    1\n",
       "13    7\n",
       "14    3\n",
       "15    7\n",
       "16    4\n",
       "17    3\n",
       "18    5\n",
       "19    7\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of train rows: {}'.format(trn_newsgroup.shape[0]))\n",
    "print('Number of train columns: {}'.format(trn_newsgroup.shape[1]))\n",
    "print('Number of test rows: {}'.format(tst_newsgroup.shape[0]))\n",
    "print('Number of test columns: {}'.format(tst_newsgroup.shape[1]))\n",
    "trn_newsgroup.describe()\n",
    "tst_newsgroup.describe()\n",
    "\n",
    "zeros = 0\n",
    "ones = 0 \n",
    "twos = 0\n",
    "threes = 0\n",
    "fours = 0\n",
    "fives = 0\n",
    "sixes = 0\n",
    "sevens = 0\n",
    "for row in trn_newsgroup_targets:\n",
    "    if(row == 0): \n",
    "        zeros = zeros + 1 \n",
    "    if(row == 1): \n",
    "        ones = ones + 1\n",
    "    if(row == 2): \n",
    "        twos = twos + 1\n",
    "    if(row == 3): \n",
    "        threes = threes + 1\n",
    "    if(row == 4): \n",
    "        fours = fours + 1\n",
    "    if(row == 5): \n",
    "        fives = fives + 1\n",
    "    if(row == 6): \n",
    "        sixes = sixes + 1\n",
    "    if(row == 7): \n",
    "        sevens = sevens + 1\n",
    "print(zeros)\n",
    "print(ones)\n",
    "print(twos)\n",
    "print(threes)\n",
    "print(fours)\n",
    "print(fives)\n",
    "print(sixes)\n",
    "print(sevens)\n",
    "print(zeros+ones+twos+threes+fours+fives+sixes+sevens)\n",
    "trn_newsgroup_targets.head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ==== Question 1.1 ====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the training set, we see that there are 5648 entries of which 1000 words and their frequencies are being checked. Obviously when dealing with large texts the word frequencies will often be 0 or extremely low. This is the case in our data set as the mean is 0.002292. There are 8 possible classes that the classifier can label an entry as. And from looking at the distribution of these every class seems to be represented quite evenly(~735 each) with the exception of class label 7 which is represented by about 300 less entries. This can lead to the classsifier being more likely to classify a class 7 entry in some other class depending on the algorithm being used for classificiation. Such as one that takes priors into account. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ==== Question 1.2 ===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing has 1883 entries of newsgroup documents. Resulting in a 75 to 25 percent split in training to testing data. This decision to have significantly more training entries brings its drawbacks and benefits. The larger you make a training set the more information the classifier will have to improve its accuracy during testing and in practice. However, the more testing data you have, the more we understand how accurate the classifier will be in practice. Essentially in prioritising training, we are gaining a more accurate predictor while losing our ability to know how  accurate the classifier will be on previously unseen entries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ==== Question 1.3 ===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating TF-IDF as opposed to just TF is very important, as this accounts for the commonality of a term when determining how important a word is in classifying the document. Common words such as 'is', 'and', 'of', etc are extremely common in all manner of documents, but gives us little to no information on what a documents classification should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Unsupervised Learining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ==== Question 2.1 ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
